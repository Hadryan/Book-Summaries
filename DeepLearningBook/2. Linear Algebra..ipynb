{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Book: Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear algebra is a branch of mathematics that is widely used throughout science and engineering. A good understanding of linear algebra is essential for understanding and working with many machine learning algorithms, especially deep learning algorithms. If you have previous experience with these concepts but need a detailed reference sheet to review key formulas, see [The Matrix CookBook](http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf). For full book about linear algebra - [Shilov 1977 linear algebra](https://cosmathclub.files.wordpress.com/2014/10/georgi-shilov-linear-algebra4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chapter is divided into following sections and subsections\n",
    "\n",
    "* Scalars, Vectors, Matrices and Tensors \n",
    "* Multiplying Matrices and Vectors \n",
    "* Identity and Inverse Matrices \n",
    "* Linear Dependence and Span \n",
    "* Norms \n",
    "* Special Kinds of Matrices and Vectors \n",
    "* Eigendecomposition \n",
    "* Singular Value Decomposition (SVD) \n",
    "* The Moore Penrose Pseudoinverse \n",
    "* The Trace Operator \n",
    "* The Determinant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scalars, Vectors, Matrices, Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The study of linear algebra involves several types of mathematical objects:\n",
    "\n",
    "\n",
    "* A scalar is a single number\n",
    "* A vector is an array of numbers of the same type (e.g. $x$ ∈ ℝ)\n",
    "* A matrix is a 2-D array\n",
    "* An array of numbers arranged in a regular grid with a variable number of axes is known as a tensor. The element at coordinates $(i, j, k)$ of a tensor $A$ is represented as $A_{i, j, k}$. To simplify atensor is a $n$-dimensional array with $n>2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://refactored.ai/track/python-for-machine-learning/images/linear-algebra1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important operation on matrices is thetranspose. The transpose of amatrix is the mirror image of the matrix across a diagonal line, called the main diagonal, running down and to the right, starting from its upper left corner. See figure below to understand it better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ![image](http://xaktly.com/Images/Mathematics/MatrixAlgebra/MatrixOperations/MatrixTranspose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With transposition you can convert a row vector to a column vector and vice versa:\n",
    "* The transpose ${A}^{\\text{T}}$ of the matrix  ${A}$ corresponds to the mirrored axes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 5],\n",
       "       [2, 4, 6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_t = A.T\n",
    "A_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of a matrix A and a vector b, yields another matrix: C= A+b, where C$_{i,j}$= A$_{i,j}$+b$_{j}$. In other words, the vector b is added to each row of the matrix. This shorthand eliminates the need to deﬁne a matrix with b copied into each row before doing the addition. This implicit copying of b to many locations is called <b>broadcasting.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "    A_{1,1}  &  A_{1,2} \\\\\\\\\n",
    "    A_{2,1}  &  A_{2,2} \\\\\\\\\n",
    "    A_{3,1}  &  A_{3,2}\n",
    "\\end{bmatrix}+\n",
    "\\begin{bmatrix}\n",
    "    B_{1,1} \\\\\\\\\n",
    "    B_{2,1} \\\\\\\\\n",
    "    B_{3,1}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is equivalent to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "    A_{1,1} & A_{1,2} \\\\\\\\\n",
    "    A_{2,1} & A_{2,2} \\\\\\\\\n",
    "    A_{3,1} & A_{3,2}\n",
    "\\end{bmatrix}+\n",
    "\\begin{bmatrix}\n",
    "    B_{1,1} & B_{1,1} \\\\\\\\\n",
    "    B_{2,1} & B_{2,1} \\\\\\\\\n",
    "    B_{3,1} & B_{3,1}\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "    A_{1,1} + B_{1,1} & A_{1,2} + B_{1,1} \\\\\\\\\n",
    "    A_{2,1} + B_{2,1} & A_{2,2} + B_{2,1} \\\\\\\\\n",
    "    A_{3,1} + B_{3,1} & A_{3,2} + B_{3,1}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting is more than just a notation convenience. When using libraries such as numpy and Tensorflow, broadcasting can reduce the memory requirements of a program. [Broadcasting in Numpy](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html) has many examples and explainations for you to refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([[1], [2], [3]])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [5, 6],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Broadcasting\n",
    "C=A+B\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multiplying Matrices and Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important operations involving matrices is multiplication of two matrices. The matrix product of matrices A and B is a third matrix C. For a matrix multiplication between two matrices $A_{mn}$ and $B_{kp}$ to exist, we must have $n == k$. The resulting matrix $C$ has the shape $m$ x $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Element-wise product</b> or <b> Hadamard Product </b> : $A \\odot B$ <br>\n",
    "<b> Dot Product </b> :  $x^Ty$ ($x$ and $y$ are of same dimension)\n",
    "\n",
    "Some properties of matrix multiplication are:\n",
    "\n",
    "* $A(B+C) = AB + AC$ (Distributive)\n",
    "* $A(BC) = (AB)C$ (Associative)\n",
    "* $AB \\ne BA$ (not commutative, in general)\n",
    "* $(AB)^T = B^TA^T$\n",
    "* $x^Ty = (x^Ty)^T = y^Tx$\n",
    "\n",
    "System of linear equation $ Ax = B $ can be written as \n",
    "\n",
    "$A_{i,1}x_1 + A_{i,2}x_2 + ... + A_{i,n}x_n = b_i$ <br>\n",
    "where $A \\in ℝ^{mxn}$ & $b \\in ℝ^{m}$ are known and $x \\in ℝ^{n}$ is to be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identity and Inverse Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation $ Ax = B $ can be solved analytically for many values of A using **Matrix Inversion**\n",
    "\n",
    "An **identity matrix** is a matrix that does not change any vector when we multiply that vector by that matrix. The entries along its main diagonal is 1. All other entries are zero.\n",
    "\n",
    "*Change the value in parantheses of the below code to take a look at n-dim identity matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **matrix inverse** of $A$ is denoted as $A^{-1}$ and is given by \n",
    "$$A^{-1}A = AA^{-1} = I_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can solve the equation $Ax = B$ using following steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  Ax = b \\\\ \n",
    "A^{-1}Ax = A^{-1}b \\\\\n",
    "I_nx = A^{-1}b \\\\ \n",
    "x = A^{-1}b \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Linear Dependence and Span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $A^{-1}$ to exist, equation $Ax = B$ must have exactly one solution for every real value of $b$. The system can have zero or infinitely many solutions for some values of b, but can't have a finite number of solutions greater than one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A_{1,1}x_1 + A_{1,2}x_2 + \\cdots + A_{1,n}x_n = b_1 \\\\\\\\\n",
    "A_{2,1}x_1 + A_{2,2}x_2 + \\cdots + A_{2,n}x_n = b_2 \\\\\\\\\n",
    "\\cdots \\\\\\\\\n",
    "A_{m,1}x_1 + A_{m,2}x_2 + \\cdots + A_{m,n}x_n = b_n\n",
    "$$\n",
    "\n",
    "So we have multiple equations with multiple unknowns. We know $A_{1,1}...A_{m,n}$ and $b_1...b_n$. To solve the system we need to find the values of the variables $x_1...x_n$ that satisfies all equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why there can't be more than 1 solution and less than an infinite number of solutions ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because we deal with **linear** systems! Two lines can't cross more than once. For visualization, let's take two dimensions and two equations. The solutions of the system correspond to the intersection of the lines. One option is that the two lines never cross (parallel). Another option is that they cross once. And finally, the last option is that they cross everywhere (superimposed). Two lines can't cross each other more than once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://8thgradevocabbook.weebly.com/uploads/1/6/9/6/16969092/864202982_orig.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, a linear combination of some set of vectors ${v(1), . . . , v(n)}$ is given by multiplying each vector $v(i)$ by a corresponding scalar coefficient and adding the results. The equation $Ax$ can be represented using matrix as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Ax = \\begin{bmatrix} {A_{1,1}x_1 + A_{1, 2}x_2 + ... A_{1, n}x_n} \\\\ {A_{2,1}x_1 + A_{2, 2}x_2 + ... A_{2, n}x_n } \\\\ . \\\\ . \\\\ . \\\\ {A_{m,1}x_1 + A_{m, 2}x_2 + ... A_{m, n}x_n} \\end{bmatrix} $$\n",
    "Which can also be written as: $$ \\begin{bmatrix} {A_{1,1}x_1 + A_{1, 2}x_2 + ... A_{1, n}x_n} \\\\ {A_{2,1}x_1 + A_{2, 2}x_2 + ... A_{2, n}x_n } \\\\ . \\\\ . \\\\ . \\\\ {A_{m,1}x_1 + A_{m, 2}x_2 + ... A_{m, n}x_n} \\end{bmatrix} = x_1 \\begin{bmatrix} A_{1,1} \\\\ A_{2,1}\\\\ . \\\\ . \\\\ . \\\\ A_{m,1}  \\end{bmatrix} + x_2 \\begin{bmatrix} A_{1,2} \\\\ A_{2,2}\\\\ . \\\\ . \\\\ . \\\\ A_{m,2}  \\end{bmatrix} + ... x_n \\begin{bmatrix} A_{1,n} \\\\ A_{2,n}\\\\ . \\\\ . \\\\ . \\\\ A_{m,n}  \\end{bmatrix} = \\sum_{i=1}^{n} x_iA_{:, i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which corresponds to the set of linear equations\n",
    "$$\n",
    "A_{1,1}x_1 + A_{1,2}x_2 + \\cdots + A_{1,n}x_n = b_1 \\\\\\\\\n",
    "A_{2,1}x_1 + A_{2,2}x_2 + \\cdots + A_{2,n}x_n = b_2 \\\\\\\\\n",
    "\\cdots \\\\\\\\\n",
    "A_{m,1}x_1 + A_{m,2}x_2 + \\cdots + A_{m,n}x_n = b_n\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus: $$ Ax = \\sum_{i=1}^{n} x_iA_{:, i}  $$\n",
    "\n",
    "Where $A_{:, i}$ is the $i^{th}$ column of $A$. This kind of operation is called a **linear combination**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **span** of a set of vectors is the set of all points obtainable by linear combination of the original vectors. Determining whether $Ax=b$ has a solution thus amounts to testing whether $b$ is in the span of the columns of $A$. This particular span is known as the column space, or the range, of A. The requirement that the column space of $A$ be all of $R^m$ implies immediately that $A$ must have at least $m$ columns, that is, $n ≥ m$. Otherwise, the dimensionality of the column space would be less than $m$.\n",
    "\n",
    "Having $n ≥ m$ is only a necessary condition for every point to have a solution.It is not a suﬃcient condition, because it is possible for some of the columns to be redundant.\n",
    "\n",
    "Here, the concept of **linear dependency** is introduced. A set of vectors are linearly independent if none of the vectors is a linear combination of the other vectors. Thus, for the column space of $A$ to be all of $R^m$, it must contain atleast one set of m linearly independent columns. The system must have at most one solution for each value of $b$, hence, $A$ must have at most m columns, or else there are multiple ways of parameterizing each solution.\n",
    "\n",
    "Together, this means that the matrix must be square, that is, we require that $m=n$ and that all the columns be linearly independent. A square matrix with linearly dependent columns is known as **singular**.\n",
    "\n",
    "If $A$ is not square yet singular, it's still possible to solve the equation, but we cannot use the method of matrix inversion for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of a vector is given by functions called **norms**.Here is the recipe to get the $p$-norm of a vector:\n",
    "\n",
    "* Calculate the absolute value of each element\n",
    "* Take the power $p$ of these absolute values\n",
    "* Sum all these powered absolute values\n",
    "* Take the power $\\frac{1}{p}$ of this result\n",
    "\n",
    "Most commonly used, the $L^p$ norm given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ ||\\mathbf{x}||_p = (\\sum_{i} |x_i|^p)^{\\frac{1}{p}} $$\n",
    "\n",
    "A norm function $f$ satisfies the following properties:\n",
    "\n",
    "$f(x) = 0 \\Rightarrow x = 0$<br>\n",
    "$f(x+y) \\leq f(x) + f(y)$ (The triangle inequality)<br>\n",
    "$\\forall \\alpha \\in ℝ, \\hspace{.1cm} f(\\alpha x) = |\\alpha|f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Euclidean Norm: This is the $L^2$ norm, which is heavily used in machine learning, and can be also calculated as $x^Tx$. \n",
    "The $L^2$ norm can be calculated with the *linalg.norm* function from numpy. We can check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm([3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the vector is in a 2-dimensional space but this stands also for more dimensions.\n",
    "\n",
    "$$\n",
    "u=\n",
    "\\begin{bmatrix}\n",
    "    u_1\\\\\\\\\n",
    "    u_2\\\\\\\\\n",
    "    \\cdots \\\\\\\\\n",
    "    u_n\n",
    "\\end{bmatrix}\n",
    "$$$$\n",
    "||u||_2 = \\sqrt{u_1^2+u_2^2+\\cdots+u_n^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L1 Norm: It is used when the difference between the zero and non-zero elements is very important.\n",
    "* Max Norm (also known as the $L^{\\infty}$ norm): Absolute value of the largest magnitude in the vector: $||x||_{\\infty} = \\displaystyle \\max_{i}|x_i|$\n",
    "* Frobenius Norm: Used to measure the size of a matrix (similar to the $L^2$ norm): $||A||_F = \\sqrt{\\displaystyle \\sum_{i,j} A_{i,j}^2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Special Kinds of Matrices and Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some special kinds of matrices and vectors are particularly useful\n",
    "\n",
    "**Diagonal Matrices:** A matrix $D$ is diagonal if and only if $Di,j= 0$ for all i =/=j. e.g. Identity Matrix.\n",
    "* A square diagonal matrix can be represented as: $diag(v)$ where the vector $v$ represents the elements along tha main diagonal.\n",
    "* Multiplying by a diagonal matrix is computationally efficient. $Dx$ can be calculated by simply scaling each $x_i$ by $v_i$.\n",
    "* A diagonal matrix need not be square\n",
    "\n",
    "**Symmetric Matrix** : $A = A^T$\n",
    "\n",
    "**Unit Vector**: A vector with **unit norm**  $||x||_2 = 1$.\n",
    "\n",
    "**Orthogonal vectors:** Two vectors $x$ and $y$ are orthogonal if $x^Ty = 0$, which means that if both of them have non-zero norm, these vectors are at a 90 degree angle to each other. **Orthogonal vectors having unit norm are called orthonormal vectors.**\n",
    "\n",
    "**Orthogonal Matrix**: A matrix whose rows are mutually orthonormal (and columns too). Thus: $$ A^TA = AA^T = I $$$$\\Rightarrow A^{-1} = A^T $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eigen-decomposition** decompose a matrix into a set of **eigenvectors** and **eigenvalues.** An eigenvector of a square matrix $A$ is a non-zero vector $v$ such that multiplication by $A$ alters only the scale of $v$:\n",
    "$$ Av = \\lambda v $$\n",
    "The scalar $\\lambda$ is called the eigenvalue corresponding to the eigenvector. Any scaled version of an eigenvector is also an eigenvector with the same eigenvalue, hence we focus on only unit eigenvectors.\n",
    "\n",
    "The eigendecomposition of a matrix $A$, having $n$ linearly independent eigenvectors represented as a matrix $V = [v^{(1)}, ... , v^{(n)}]$ with the corresponding eigenvalues given by the vector $\\lambda = [\\lambda_1, ... , \\lambda_n]$ is given by:\n",
    "\n",
    "$$ A = Vdiag(\\lambda)V^{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://www.simplilearn.com/ice9/free_resources_article_thumb/effect-of-eigenvalue-and-eigenvectors.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **singular value decomposition(SVD)** provides another way to factorizea matrix, into singular vectors and singular values. The SVD enables us to discover some of the same kind of information as the eigen-decomposition reveals;however, the SVD is more generally applicable.\n",
    "\n",
    "For SVD, we represent $A$ as \n",
    "$$A = UDV^T $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $A$ is an $mxn$ matrix. Then $U$ is defined to be an $mmn$ matrix, $D$ to be an $mxn$ matrix and $V$ to be an $nxn$ matrix.\n",
    "\n",
    "The matrices $U$ and $V$ are both defined to be orthogonal matrices. Matrix $D$ is defined to be a diagonal matrix (not necessarily square). The elements along the diagonal of $D$ are known as the **singular values** ofthe matrix $A$. The columns of $U$ are known as the **left-singular vectors**. The columns of $V$ are known as as the **right-singular vector**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. . The Moore-Penrose Pseudoinverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to make a left-inverse $B$ of a matrix $A(mxn)$ so that we can solve a linear equation\n",
    "$$ Ax = y $$$$\\Rightarrow x = By$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the pseudoinverse of $A$ as:\n",
    "\n",
    "$$A^+ = \\lim\\limits_{\\alpha \\rightarrow 0} (A^TA + \\alpha I)^{-1}A^T$$\n",
    "However, for practical algorithms its defined as:\n",
    "\n",
    "$$ A^+ = VD^+U^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $U$,$D$ and $V$ are the singular value decomposition of $A$, and the pseudoinverse $D+$ of a diagonal matrix $D$ is obtained by taking the reciprocal of its non-zero elements then taking the transpose of the resulting matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When m>n, it is possible for there to be no solution and $A^+$ gives the $x$ such that $Ax$ is as close to $y$ in terms of the Euclidean norm $||Ax - y||$.\n",
    "\n",
    "When m<=n, using $A^+$, gives one of many possible solutions, with the minimal Euclidean norm:\n",
    "\n",
    "$$ x = A^{+}y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. The Trace Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The trace operator gives the sum of all the diagonal elements.\n",
    "\n",
    "$$ Tr(A) = \\sum_{i}A_{i,i}$$\n",
    "\n",
    "\n",
    "\n",
    "$||A||_F = \\sqrt{Tr(AA^T)} $ (Frobenius Norm)<br>\n",
    "$Tr(A) = Tr(A^T)$ (Transpose Invariance)<br>\n",
    "$Tr(ABC) = Tr(CAB) = Tr(BCA)$ <br>( If the shapes of corresponding matrices allow it)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. The Determinant### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The determinant of a square matrix (denoted by $det(A)$) maps matrices to real scalars. It is equal to the product of all the eigenvalues of the matrix. It denotes how much multiplication by the matrix expands or contracts space. If the value is 0, then space is contracted completely atleast along one dimension causing it to lose all its volume. If the value is 1, then the transformation preserves volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've explained PCA in detail [here](https://medium.com/@the_change/principal-component-analysis-explained-560df8832b93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
